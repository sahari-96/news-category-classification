# -*- coding: utf-8 -*-
"""Untitled158.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qHMsxDL6Hr9I3qyOVW_A-SdtVTAv7RlH
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os
import random
import re
from sklearn import metrics

!pip install transformers
from transformers import RobertaForSequenceClassification, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup, BertForMultipleChoice
import torch
import torch.nn as nn
from torch.nn.modules.adaptive import AdaptiveLogSoftmaxWithLoss
from torch.nn import LogSoftmax
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

import time
import datetime


if torch.cuda.is_available():    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

#loading data
training_data = pd.read_csv("/content/drive/MyDrive/train.csv", index_col = 0)
test_data = pd.read_csv("/content/drive/MyDrive/test.csv", index_col = 0)

training_label = training_data["label"].values
training_content = training_data["cleaned_content"].values.astype("str")
training_title = training_data['cleaned_title'].values.astype("str")
training_author = training_data["author"].values.astype("str")
training_date = training_data["date"].values


test_label = test_data["label"].values
test_content = test_data["cleaned_content"].values.astype("str")
test_title = test_data['cleaned_title'].values.astype("str")
test_author = test_data["author"].values.astype("str")
test_date = test_data["date"].values


print("finish loading data")

#preprocess data
#define tokenizer
training_text = np.empty(len(training_label), dtype = "object")
test_text = np.empty(len(test_label), dtype = "object")

n = len(training_text)
for i in range(n):
    training_text[i] = str(training_content[i])  +  " " + str(training_title[i]) 
   
        
n = len(test_text)
for i in range(n):
    test_text[i] = str(test_content[i])  +  " " + str(test_title[i])
        

tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case = True)

#tokenize descriptions

train_input_ids = []
train_attention_masks = []
for sent in training_text:
    encoded = tokenizer.encode_plus(
                        sent,                      # Sentence to encode.
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 128,           # Pad & truncate all sentences.
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Construct attn. masks.
                        return_tensors = 'pt',     # Return pytorch tensors.
                   )
    train_input_ids.append(encoded['input_ids'])
    train_attention_masks.append(encoded['attention_mask'])

    
train_inputs = torch.cat(train_input_ids, dim=0)
train_masks = torch.cat(train_attention_masks, dim=0)
train_labels = torch.tensor(training_label, dtype = torch.long) 


test_input_ids = []
test_attention_masks = []
for sent in test_text:
    encoded = tokenizer.encode_plus(
                        sent,                      # Sentence to encode.
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 128,           # Pad & truncate all sentences.
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Construct attn. masks.
                        return_tensors = 'pt',     # Return pytorch tensors.
                   )
    test_input_ids.append(encoded['input_ids'])
    test_attention_masks.append(encoded['attention_mask'])

validation_inputs = torch.cat(test_input_ids, dim=0)
validation_masks = torch.cat(test_attention_masks, dim=0)
validation_labels = torch.tensor(test_label, dtype = torch.long)

# batch_size 32 too large for memory
batch_size = 16

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

# initialize model
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 2, output_attentions = False, output_hidden_states = False)

model.to(device)


# literature values for optimizer
optimizer = AdamW(model.parameters(), lr = 3e-5, eps =1e-8)


epochs = 3

# Total number of training steps
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = total_steps*0.06,
                                            num_training_steps = total_steps)



# calculate (number of correct labels)/(total number of labels)

from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import confusion_matrix

# Function to calculate the accuracy of our predictions vs labels
def flat_score(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    score = precision_recall_fscore_support(labels_flat, pred_flat, average = None)
    return np.sum(pred_flat == labels_flat) / len(labels_flat), score[0][1], score[1][1], score[2][1]


def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

def write_to_file(filename, string):
    f = open(filename, "a")
    f.write(string + '\n')
    f.close()



import random


seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

loss_values = []

# training model
for epoch_i in range(0, epochs):
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    write_to_file('roberta.txt', '======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))

    print('Training...')
    t0 = time.time()

    total_loss = 0
    model.train()

    for step, batch in enumerate(train_dataloader):
        if step % 100 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        # no gradient accumulation
        model.zero_grad()

        outputs = model(b_input_ids,
                    token_type_ids=None,
                    attention_mask=b_input_mask,
                    labels=b_labels)
        
        # outputs is a tuple
        loss = outputs[0]
        total_loss += loss.item()
        loss.backward()

        # Clip the gradient norm to 1.0 (for exploding gradient)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_loss / len(train_dataloader)    
    
    loss_values.append(avg_train_loss)

    print("")
    print(" Average training loss: {:}".format(avg_train_loss))
    print(" Training epoch took: {:}".format(format_time(time.time() - t0)))
    print("")
    print("Running Validation...")
    write_to_file('roberta.txt', "Average training loss: {:}".format(avg_train_loss))
    torch.save(model.state_dict(),'roberta' + str(epoch_i) + '.pt')
    t0 = time.time()
    model.eval()
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    preds_all, labels_all = [], []
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0
    precision = 0
    recall = 0
    f1_score = 0
    
    #validation step
    for step, batch in enumerate(validation_dataloader):
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        with torch.no_grad():        
            outputs = model(b_input_ids, 
                            token_type_ids=None, 
                            attention_mask=b_input_mask)
        logits = outputs[0]

        # Move logits and labels to CPU and accumulate predictions and labels
        logits = logits.detach().cpu().numpy()
        preds_all += np.argmax(logits, axis=1).flatten().tolist()
        label_ids = b_labels.to('cpu').numpy()
        labels_all += label_ids.tolist()
        
        # Calculate the accuracy for this batch of test sentences.
        scores = flat_score(logits, label_ids)
        
        # Accumulate the total accuracy.
        total_eval_accuracy += scores[0]
        precision += scores[1]
        recall += scores[2]
        f1_score += scores[3]

        # Track the number of batches
        nb_eval_steps += 1
        if nb_eval_steps % 100 == 0:
            print("Validation step: {:}".format(nb_eval_steps))

    # Report the final accuracy for this validation run.
    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
    print("  Accuracy: {0:.5f}".format(avg_val_accuracy))
    print("  Precision: {0:.5f}".format(precision))
    print("  Recall: {0:.5f}".format(recall))
    print("  f1-score: {0:.5f}".format(f1_score))

    write_to_file('roberta.txt', "  Accuracy: {:}".format(avg_val_accuracy))
    write_to_file('roberta.txt', "  Precision: {:}".format(precision))
    write_to_file('roberta.txt', "  Recall: {:}".format(recall))
    write_to_file('roberta.txt', "  f1-score: {:}".format(f1_score))
    print("  Validation took: {:}".format(format_time(time.time() - t0)))

print("")
print("Training complete!")